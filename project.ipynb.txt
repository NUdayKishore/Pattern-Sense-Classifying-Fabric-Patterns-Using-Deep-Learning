{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1dd-acif1g5Ih1mJajcGGd0jXybmk7GNG","authorship_tag":"ABX9TyPHipUBaoDSRFU1ERUcjQyd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import files\n","files.upload()\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"wcYN_oIdoe1f","executionInfo":{"status":"ok","timestamp":1751283922454,"user_tz":-330,"elapsed":6972,"user":{"displayName":"chaitanya","userId":"04429170506741360883"}},"outputId":"9bdfee6d-d1c9-48b7-e732-b33194d1ca7f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-e9af8354-ecc6-4008-ba59-61018afc4a53\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-e9af8354-ecc6-4008-ba59-61018afc4a53\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]}]},{"cell_type":"code","source":["!kaggle datasets download -d nguyngiabol/dress-pattern-dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zy8gcAyCp_-P","executionInfo":{"status":"ok","timestamp":1751283947089,"user_tz":-330,"elapsed":8628,"user":{"displayName":"chaitanya","userId":"04429170506741360883"}},"outputId":"d55b5455-81fa-403e-c93d-e9eb2febd592"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/nguyngiabol/dress-pattern-dataset\n","License(s): unknown\n","Downloading dress-pattern-dataset.zip to /content\n","  0% 0.00/118M [00:00<?, ?B/s]\n","100% 118M/118M [00:00<00:00, 1.32GB/s]\n"]}]},{"cell_type":"code","source":["!unzip -q dress-pattern-dataset.zip -d '/content/drive/MyDrive/data_sets'"],"metadata":{"id":"62agFe0hqLPH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","# Base directory where folders are stored\n","base_path = '/content/drive/MyDrive/data_sets/data_pattern'\n","\n","# Collect image paths and labels\n","data = []\n","for label in os.listdir(base_path):\n","    folder_path = os.path.join(base_path, label)\n","    for file in os.listdir(folder_path):\n","        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n","            data.append({'file_path': os.path.join(folder_path, file), 'label': label})\n","\n","df = pd.DataFrame(data)\n"],"metadata":{"id":"7uVtqzOpqdQ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# First split: 80% training, 20% temp (val + test)\n","train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n","\n","# Second split: 10% validation, 10% test\n","valid_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n","\n","# Confirm the sizes\n","print(f\"Training: {len(train_df)}, Validation: {len(valid_df)}, Test: {len(test_df)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWL71rr9qhHO","executionInfo":{"status":"ok","timestamp":1751368771635,"user_tz":-330,"elapsed":250,"user":{"displayName":"chaitanya","userId":"04429170506741360883"}},"outputId":"4116e39e-4eda-4443-a185-4fca25798612"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training: 3384, Validation: 423, Test: 423\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import random\n"],"metadata":{"id":"Jkdv_eUdqmHA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def apply_transform(image):\n","    # Random rotation between -40 to 40 degrees\n","    angle = random.uniform(-40, 40)\n","    h, w = image.shape[:2]\n","    M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)\n","    image = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n","\n","    # Random horizontal flip (50% chance)\n","    if random.random() > 0.5:\n","        image = cv2.flip(image, 1)\n","\n","    # Random vertical flip (50% chance)\n","    if random.random() > 0.5:\n","        image = cv2.flip(image, 0)\n","\n","    # Random brightness/contrast adjustment\n","    alpha = random.uniform(0.8, 1.2)  # contrast control\n","    beta = random.randint(-20, 20)    # brightness control\n","    image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n","\n","    # Random gamma correction\n","    gamma = random.uniform(0.7, 1.5)\n","    invGamma = 1.0 / gamma\n","    table = np.array([(i / 255.0) ** invGamma * 255 for i in range(256)]).astype(\"uint8\")\n","    image = cv2.LUT(image, table)\n","\n","    return image"],"metadata":{"id":"SjJRtkNTquSI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def apply_augmentation(image_path, label):\n","    # Read and convert image from BGR to RGB\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        raise FileNotFoundError(f\"Image not found at {image_path}\")\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # Apply transformation\n","    aug_image = apply_transform(image)\n","\n","    return aug_image, label\n"],"metadata":{"id":"kY0mrt-gqzs2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"metadata":{"id":"qlikkrWsq2n_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datagen = ImageDataGenerator(\n","    rescale=1./255,                      # Normalize pixel values\n","    width_shift_range=0.1,              # Horizontal shift (10%)\n","    height_shift_range=0.1,             # Vertical shift (10%)\n","    horizontal_flip=True,               # Random horizontal flip\n","    vertical_flip=True,                 # Random vertical flip\n","    rotation_range=40,                  # Rotate within ±40 degrees\n","    brightness_range=[0.8, 1.2],        # Adjust brightness\n","    zoom_range=0.2                      # Zoom in/out by 20%\n",")"],"metadata":{"id":"2EeWfby5q7Tn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_generator = datagen.flow_from_dataframe(\n","    dataframe=train_df,\n","    x_col='file_path',\n","    y_col='label',\n","    target_size=(224, 224),  # Resize all images\n","    batch_size=32,\n","    class_mode='categorical'  # or 'binary' / 'sparse'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ju5KpXvlq_Ze","executionInfo":{"status":"ok","timestamp":1751368782747,"user_tz":-330,"elapsed":565,"user":{"displayName":"chaitanya","userId":"04429170506741360883"}},"outputId":"297dcc61-4dd7-49e9-d80c-21807f1ecbde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3384 validated image filenames belonging to 10 classes.\n"]}]},{"cell_type":"code","source":["valid_generator = datagen.flow_from_dataframe(\n","    dataframe=valid_df,\n","    x_col='file_path',\n","    y_col='label',\n","    target_size=(224, 224),  # Resize all images\n","    batch_size=32,\n","    class_mode='categorical'  # or 'binary' / 'sparse'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5DtiS4CBrDPn","executionInfo":{"status":"ok","timestamp":1751368784220,"user_tz":-330,"elapsed":80,"user":{"displayName":"chaitanya","userId":"04429170506741360883"}},"outputId":"20c683e7-d6e1-4360-a78a-d02238970282"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 423 validated image filenames belonging to 10 classes.\n"]}]},{"cell_type":"code","source":["test_generator = datagen.flow_from_dataframe(\n","    dataframe=test_df,\n","    x_col='file_path',\n","    y_col='label',\n","    target_size=(224, 224),  # Resize all images\n","    batch_size=32,\n","    class_mode='categorical'  # or 'binary' / 'sparse'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0iz8InHrrGQN","executionInfo":{"status":"ok","timestamp":1751368787194,"user_tz":-330,"elapsed":75,"user":{"displayName":"chaitanya","userId":"04429170506741360883"}},"outputId":"6b880bb3-6754-41ac-bf1f-6f46854951d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 423 validated image filenames belonging to 10 classes.\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam"],"metadata":{"id":"UH-Lfp2XrJoe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load base MobileNetV2 without top layers\n","base_model = MobileNetV2(\n","    input_shape=(224, 224, 3), # Change input shape to match the data generator\n","    include_top=False,\n","    weights='imagenet'\n",")\n","\n","# Freeze base model layers\n","base_model.trainable = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_5gnrw9_rNzu","executionInfo":{"status":"ok","timestamp":1751352432335,"user_tz":-330,"elapsed":4509,"user":{"displayName":"chaitanya","userId":"04429170506741360883"}},"outputId":"e1adf284-9a49-44af-f49e-09efbb38f456"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"]}]},{"cell_type":"code","source":["import os\n","\n","directory = '/content/drive/MyDrive/data_sets/data_pattern'\n","labels = os.listdir(directory)\n","labels.sort()\n","print(\"Class labels:\", labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8Rv0eSMrdN3","executionInfo":{"status":"ok","timestamp":1751352435456,"user_tz":-330,"elapsed":5,"user":{"displayName":"chaitanya","userId":"04429170506741360883"}},"outputId":"41a2725c-5f38-4812-bd12-881646a9320b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class labels: ['animal', 'cartoon', 'floral', 'geometry', 'ikat', 'plain', 'polka dot', 'squares', 'stripes', 'tribal']\n"]}]},{"cell_type":"code","source":["x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dropout(0.3)(x)\n","x = Dense(128, activation='relu')(x)\n","x = Dropout(0.2)(x)\n","NUM_CLASSES = len(labels)\n","output = Dense(NUM_CLASSES, activation='softmax')(x)\n","\n","model = Model(inputs=base_model.input, outputs=output)"],"metadata":{"id":"R9Q-GBZ_rUB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","from tensorflow.keras.optimizers import Adam\n","\n","# Load the pre-trained model\n","model = load_model(\"/content/drive/MyDrive/best_model.h5\")\n","\n","# Re-compile the model after loading\n","model.compile(\n","    optimizer=Adam(learning_rate=1e-4),\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKox1QaSvaKP","executionInfo":{"status":"ok","timestamp":1751368734460,"user_tz":-330,"elapsed":982,"user":{"displayName":"chaitanya","userId":"04429170506741360883"}},"outputId":"5c97f1b0-22a0-4125-8e12-724b327a5d7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","checkpoint = ModelCheckpoint(\n","    filepath='/content/drive/MyDrive/best.h5',      # file to save the best model\n","    monitor='accuracy',            # monitor validation loss\n","    save_best_only=True,           # only save the best model\n","    mode='max',                    # minimize val_loss\n","    verbose=1\n",")\n"],"metadata":{"id":"YnqJh_-LsIlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    train_generator,             # Or train_ds if using tf.data\n","    validation_data=valid_generator,\n","    epochs=10,\n","    callbacks=[checkpoint],\n","    verbose=1\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IQJKizfxrmMe","outputId":"3b376cf9-805b-4275-f9db-2bf94c34e22e","executionInfo":{"status":"ok","timestamp":1751370990339,"user_tz":-330,"elapsed":2195729,"user":{"displayName":"chaitanya","userId":"04429170506741360883"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.4948 - loss: 1.5232 "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: accuracy improved from -inf to 0.48493, saving model to /content/drive/MyDrive/best.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1553s\u001b[0m 15s/step - accuracy: 0.4947 - loss: 1.5234 - val_accuracy: 0.5012 - val_loss: 1.4440\n","Epoch 2/10\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551ms/step - accuracy: 0.4896 - loss: 1.5246\n","Epoch 2: accuracy improved from 0.48493 to 0.48848, saving model to /content/drive/MyDrive/best.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 636ms/step - accuracy: 0.4896 - loss: 1.5246 - val_accuracy: 0.5201 - val_loss: 1.4826\n","Epoch 3/10\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552ms/step - accuracy: 0.4997 - loss: 1.5187\n","Epoch 3: accuracy improved from 0.48848 to 0.49439, saving model to /content/drive/MyDrive/best.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 635ms/step - accuracy: 0.4997 - loss: 1.5188 - val_accuracy: 0.4988 - val_loss: 1.4730\n","Epoch 4/10\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542ms/step - accuracy: 0.5006 - loss: 1.5194\n","Epoch 4: accuracy improved from 0.49439 to 0.50355, saving model to /content/drive/MyDrive/best.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 625ms/step - accuracy: 0.5007 - loss: 1.5194 - val_accuracy: 0.5248 - val_loss: 1.4446\n","Epoch 5/10\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542ms/step - accuracy: 0.5036 - loss: 1.4616\n","Epoch 5: accuracy did not improve from 0.50355\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 618ms/step - accuracy: 0.5035 - loss: 1.4618 - val_accuracy: 0.5272 - val_loss: 1.4342\n","Epoch 6/10\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548ms/step - accuracy: 0.5019 - loss: 1.4754\n","Epoch 6: accuracy did not improve from 0.50355\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 622ms/step - accuracy: 0.5019 - loss: 1.4755 - val_accuracy: 0.5130 - val_loss: 1.4400\n","Epoch 7/10\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537ms/step - accuracy: 0.5072 - loss: 1.4962\n","Epoch 7: accuracy improved from 0.50355 to 0.51123, saving model to /content/drive/MyDrive/best.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 618ms/step - accuracy: 0.5072 - loss: 1.4960 - val_accuracy: 0.5437 - val_loss: 1.4333\n","Epoch 8/10\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543ms/step - accuracy: 0.5142 - loss: 1.4587\n","Epoch 8: accuracy did not improve from 0.51123\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 640ms/step - accuracy: 0.5142 - loss: 1.4589 - val_accuracy: 0.5296 - val_loss: 1.4456\n","Epoch 9/10\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553ms/step - accuracy: 0.5152 - loss: 1.4647\n","Epoch 9: accuracy improved from 0.51123 to 0.51212, saving model to /content/drive/MyDrive/best.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 639ms/step - accuracy: 0.5152 - loss: 1.4647 - val_accuracy: 0.5177 - val_loss: 1.4527\n","Epoch 10/10\n","\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541ms/step - accuracy: 0.5311 - loss: 1.4159\n","Epoch 10: accuracy improved from 0.51212 to 0.52098, saving model to /content/drive/MyDrive/best.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 623ms/step - accuracy: 0.5310 - loss: 1.4162 - val_accuracy: 0.5059 - val_loss: 1.4305\n"]}]}]}